[program:{{ sc_crawling_name }}]
command={{ sc_virtualenv_path }}/bin/scrapy runspider crawling/spiders/link_spider.py
directory= {{ sc_install_dir }}/default/crawler
environment=PATH="{{ sc_virtualenv_path }}/bin"
process_name=link_%(process_num)02d
numprocs={{ sc_crawling_num_procs }}
autostart=true
autorestart=true
startsecs={{ sc_crawling_start_secs }}
startretries={{ sc_crawling_start_retries }}
stopsignal=KILL
redirect_stderr=true
stdout_logfile=AUTO
stdout_logfile_maxbytes={{ sc_crawling_stdout_logfile_maxbytes }}
stderr_logfile=AUTO
stderr_logfile_maxbytes={{ sc_crawling_stderr_logfile_maxbytes }}
{% if sc_proxy_url is defined and sc_proxy_port is defined %}
environment = http_proxy="{{ sc_proxy_url }}:{{ sc_proxy_port }}", https_proxy="{{ sc_proxy_url }}:{{ sc_proxy_port }}"{% endif %}
